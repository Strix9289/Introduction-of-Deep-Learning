{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "initializer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyONcT6/X8zENx9A31d7ZLbS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Strix9289/Introduction-of-Deep-Learning/blob/master/initializer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNCzUxi850q2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP6m4sq36AbF",
        "colab_type": "text"
      },
      "source": [
        "# 初期化(initializer)\n",
        "各層のパラメタは0を中心とした乱数で初期化されているが、大きすぎる値の場合学習の初期段階での勾配が過大になり、\n",
        "小さすぎると勾配自体も過小になってしまい、いずれにしても学習がうまく進まない。\n",
        "\n",
        "そこで、初期化にあたっては、その値のスケール(分散)をうまく調整する必要がある。\n",
        "\n",
        "- #### LeCun : \n",
        "- #### Glorot : 活性化関数がすべて線形な場合\n",
        "- #### He : 活性化関数がReLUであるとき\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSs7pFXZ6BYI",
        "colab_type": "text"
      },
      "source": [
        "## LeCunの初期化\n",
        "\n",
        "各層の入力次元を$n_{in}$として, 次のように初期化します.\n",
        "$$\n",
        "    \\theta \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{3}{n_{\\textrm{in}}}}, \\sqrt{\\frac{3}{n_{\\textrm{in}}}}\\right) \\quad \\textrm{or} \\quad \\mathcal{N}\\left(0, \\frac{1}{\\sqrt{n_{\\textrm{in}}}}\\right)\n",
        "$$\n",
        "\n",
        "なお、$\\mathcal{U}$は一様分布、$\\mathcal{N}$は正規分布を表します。\n",
        "\n",
        "Kerasでは、それぞれ`keras.initializers.lucun_uniform`、`keras.initializers.lucun_normal`として定義されていますが、\n",
        "\n",
        "以下のようにlayerの引数としてキーワードで指定することも可能です。\n",
        "\n",
        "```py\n",
        "# LuCun's initializationの実装例\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='lucun_uniform'))\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='lucun_normal'))\n",
        "```\n",
        "\n",
        "参考：\n",
        "https://keras.io/ja/initializers/#lecun_normal\n",
        "https://keras.io/ja/initializers/#lecun_uniform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9XEagbU6EDW",
        "colab_type": "text"
      },
      "source": [
        "## Glorotの初期化（Xavierの初期化）\n",
        "\n",
        "各層の入力次元を$n_{\\textrm{in}}$, 出力次元を$n_{\\textrm{out}}$として, 次のように初期化します.\n",
        "$$\n",
        "    \\theta \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{6}{n_{\\textrm{in}}+n_{\\textrm{out}}}}, \\sqrt{\\frac{6}{n_{\\textrm{in}}+n_{\\textrm{out}}}}\\right) \\quad \\textrm{or} \\quad \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{\\textrm{in}}+n_{\\textrm{out}}}}\\right)\n",
        "$$\n",
        "\n",
        "なお、$\\mathcal{U}$は一様分布、$\\mathcal{N}$は正規分布を表します。\n",
        "\n",
        "Kerasでは、それぞれ`keras.initializers.glorot_uniform`、`keras.initializers.glorot_normal`として定義されていますが、\n",
        "\n",
        "以下のようにlayerの引数としてキーワードで指定することも可能です。\n",
        "\n",
        "```py\n",
        "# Glorot's initializationの実装例\n",
        "model.add(Dense(128, activation='sigmoid', kernel_initializer='glorot_uniform'))\n",
        "model.add(Dense(128, activation='sigmoid', kernel_initializer='glorot_normal'))\n",
        "```\n",
        "\n",
        "参考：\n",
        "https://keras.io/ja/initializers/#glorot_normal\n",
        "https://keras.io/ja/initializers/#glorot_uniform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfeWkImw6HrC",
        "colab_type": "text"
      },
      "source": [
        "## Heの初期化\n",
        "\n",
        "各層の入力次元を$n_{\\textrm{in}}$として, 次のように初期化します.\n",
        "$$\n",
        "    \\theta \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{6}{n_{\\textrm{in}}}}, \\sqrt{\\frac{6}{n_{\\textrm{in}}}}\\right) \\quad \\textrm{or} \\quad \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{\\textrm{in}}}}\\right)\n",
        "$$\n",
        "\n",
        "なお、$\\mathcal{U}$は一様分布、$\\mathcal{N}$は正規分布を表します。\n",
        "\n",
        "Kerasでは、それぞれ`keras.initializers.he_uniform`、`keras.initializers.he_normal`として定義されていますが、\n",
        "\n",
        "以下のようにlayerの引数としてキーワードで指定することも可能です。\n",
        "\n",
        "```py\n",
        "# He's initializationの実装例\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='he_normal'))\n",
        "```\n",
        "\n",
        "参考：\n",
        "https://keras.io/ja/initializers/#he_normal\n",
        "https://keras.io/ja/initializers/#he_uniform"
      ]
    }
  ]
}