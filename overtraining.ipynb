{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "overtraining.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOb/kBM73uEBHg1RfSOQOh+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Strix9289/Introduction-of-Deep-Learning/blob/master/overtraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sjZoAig6dmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDJ18OEx6ebi",
        "colab_type": "text"
      },
      "source": [
        "# 過学習\n",
        "- 学習：既知のデータに対して正確に予想すること\n",
        "- 目的：未知のデータに対して正確に予想すること\n",
        "\n",
        "学習が進みすぎると、既知のデータが持つそれ自身には意味がないような統計的な性質まで学習してしまい、精度が落ちる\n",
        "\n",
        "+ #### 正則化\n",
        "    * #### L2正則化\n",
        "    * #### L1正則化\n",
        "    * #### ElasticNet\n",
        "+ #### 早期終了\n",
        "+ #### ドロップアウト\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Scbvlhar6fWo",
        "colab_type": "text"
      },
      "source": [
        "## -正則化(regularization)\n",
        "\n",
        "使うパラメータの数をなるべく少なくなるように学習をする\n",
        "→損失関数にパラメタの大きさに対するペナルティ項(正則化項)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZvUrpTM6hqw",
        "colab_type": "text"
      },
      "source": [
        "## -L2正則化\n",
        "\n",
        "L2正則化では、全パラメータの2乗和を正則化項として損失関数に加えます。\n",
        "\n",
        "L2正則化では、パラメータを完全に0にすることは少ないものの、パラメータを滑らかにすることで予測精度のより良いモデルを構築します。\n",
        "\n",
        "Kerasでは`keras.regularizers.l2`として定義されていますが、実際には各layerにregularizerとして引数で指定して用います。\n",
        "\n",
        "なお、`keras.regularizers.l2`は引数として、正則化項に掛かる係数を指定できます。\n",
        "\n",
        "```py\n",
        "from keras.layers import Dense\n",
        "from keras import regularizers\n",
        "model.add(Dense(128, kernel_regularizer=regularizers.l2(0.01))\n",
        "```\n",
        "\n",
        "<small>\n",
        "\n",
        "<参考>\n",
        "$\\Theta$：パラメータ、$\\lambda$：係数（＝引数）\n",
        "$$\n",
        "    \\mathrm{E}(\\boldsymbol{w}) = \\sum^{N}_{n=1}\\sum^{K}_{k=1} t^{(n)}_{k} \\ln y^{(n)}_k + \\lambda \\sum_{i}w_i^2\n",
        "$$\n",
        "\n",
        "</small>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWTSjPQa6kLw",
        "colab_type": "text"
      },
      "source": [
        "## -L1正則化\n",
        "\n",
        "L1正則化では、全パラメータの絶対値の和を正則化項として損失関数に加えます。\n",
        "\n",
        "L1正則化ではL2正則化よりもパラメータが0になりやすいという特徴（**スパース性**）があります。\n",
        "\n",
        "Kerasでは`keras.regularizers.l1`として定義されていますが、実際には各layerにregularizerとして引数で指定して用います。\n",
        "\n",
        "なお、`keras.regularizers.l1`は引数として、正則化項に掛かる係数を指定できます。\n",
        "\n",
        "```py\n",
        "from keras.layers import Dense\n",
        "from keras import regularizers\n",
        "model.add(Dense(128, kernel_regularizer=regularizers.l1(0.01))\n",
        "```\n",
        "<small>\n",
        "\n",
        "<参考>\n",
        "$\\boldsymbol{w}$：パラメータ、$\\lambda$：係数（＝引数）\n",
        "$$\n",
        "    \\mathrm{E}(\\boldsymbol{w}) = \\sum^{N}_{n=1}\\sum^{K}_{k=1} t^{(n)}_{k} \\ln y^{(n)}_k + \\lambda \\sum_{i}|w_i|\n",
        "$$\n",
        "\n",
        "</small>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwH6gHP66mn9",
        "colab_type": "text"
      },
      "source": [
        "## -ElasticNet\n",
        "\n",
        "L1正則化とL2正則化の組み合わせです。\n",
        "\n",
        "Kerasでは`keras.regularizers.l1_l2`として定義されていますが、実際には各layerにregularizerとして引数で指定して用います。\n",
        "\n",
        "なお、`keras.regularizers.l1_l2`は引数として、各々の正則化項に掛かる係数を指定できます。\n",
        "\n",
        "```py\n",
        "from keras.layers import Dense\n",
        "from keras import regularizers\n",
        "model.add(Dense(128, kernel_regularizer=regularizers.l1_l2(l1=0.01,l2=0.01))\n",
        "```\n",
        "<small>\n",
        "\n",
        "<参考>\n",
        "$\\boldsymbol{w}$：パラメータ、$\\lambda$：係数、$\\alpha$：L1正則化とL2正則化の割合\n",
        "$$\n",
        "    \\mathrm{E}(\\boldsymbol{w}) = \\sum^{N}_{n=1}\\sum^{K}_{k=1} t^{(n)}_{k} \\ln y^{(n)}_k + \\lambda \\sum_{i}[\\alpha|w_i|+(1-\\alpha)w_i^2]\n",
        "$$\n",
        "よくある定義式としては上の通りですが、Kerasの実装との対応は、\n",
        "$$\n",
        "    l1=\\lambda\\alpha, \\quad l2=\\lambda(1-\\alpha) \\Leftrightarrow \\lambda=l1+l2, \\quad \\alpha = \\frac{l1}{l1+l2}\n",
        "$$\n",
        "</small>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8HxgAsG6q2A",
        "colab_type": "text"
      },
      "source": [
        "## 早期終了\n",
        "`Keras`ではcallbackという機能を使って各エポック毎のモデルにパラメータを保存し、\n",
        "\n",
        "検証データのコストが大きくなったら前のエポックのパラメータを使用するようにします。\n",
        "\n",
        "`model.fit`の引数に以下のように設定します。\n",
        "\n",
        "```py\n",
        "model.fit(x=x_train, y=y_train, ...,\n",
        "    callbacks=keras.callbacks.EarlyStopping(patience=0, verbose=1))\n",
        "```\n",
        "\n",
        "参考：https://keras.io/ja/callbacks/#earlystopping\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aQc3dw36spk",
        "colab_type": "text"
      },
      "source": [
        "## ドロップアウト\n",
        "過学習で余計に学習している部分に着目すると、それは統計的なばらつきと言えるような部分でした。\n",
        "\n",
        "こうした確率的なばらつきは、一般に大量に足し合わせると相殺されます。\n",
        "\n",
        "そこで、訓練データセットから部分訓練データセットを大量に作成し、各モデルの予測結果を平均するアンサンブルという手法が用いられることがあります。\n",
        "\n",
        "このアンサンブル法は大変魅力的な手法なのですが、とてつもない計算量を要するためそのまま用いることは難しいものでした。\n",
        "\n",
        "そこで出てきたものが、**ドロップアウト (dropout)**と呼ばれる手法で、これは近似的にアンサンブル法を実現するものになっています。\n",
        "\n",
        "具体的には、ドロップアウトは入力の一部をランダムに0にして出力するlayerの一種です。要するに一部のユニットを取り除いた状況を再現します。\n",
        "\n",
        "このユニットの除去を確率的に行い、一部のユニットが除去された部分ネットワークに対して学習することを繰り返すことで、\n",
        "\n",
        "多数のモデルを同時に訓練することと同じ効果を再現しているわけです。\n",
        "\n",
        "Kerasでは、`keras.layers.core.Dropout`クラスを用いて実装できます。\n",
        "\n",
        "```py\n",
        "keras.layers.core.Dropout(rate, noise_shape=None, seed=None)\n",
        "```\n",
        "\n",
        "主な引数は、\n",
        "\n",
        "* rate: 入力を0にする確率、0～1の実数値\n",
        "* seed: 乱数のシード値\n",
        "\n",
        "です。（入力と出力でshapeは変わりません）\n",
        "\n",
        "参考：https://keras.io/ja/layers/core/#dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WNZwW8p6tci",
        "colab_type": "text"
      },
      "source": [
        "# バッチ学習\n",
        "\n",
        "\n",
        "\n",
        "<ul>\n",
        "    <li><strong>バッチ学習（一括学習、batch learning）</strong>\n",
        "    <ul>データセット全体を一度に全て使用してパラメータを決定</ul>\n",
        "    </li>\n",
        "    <li><strong>オンライン学習（逐次学習、online learning）</strong>\n",
        "    <ul>データセット全体を持っているが、学習を1データ毎に繰り返す</ul>\n",
        "    <ul>（or データが1つずつ時系列として与えられるので、そのたびに学習を行い、パラメータを更新（ストリーム学習））</ul>\n",
        "    </li>\n",
        "</ul>\n",
        "\n",
        "オンライン学習とバッチ学習を比較すると、次のような特徴があります。\n",
        "\n",
        "<ul>\n",
        "    <li>オンライン学習では1度の学習に使用するデータが1つなので、省メモリ</li>\n",
        "    <li>バッチ学習ではスカラー計算ではなくベクトル・行列計算を行うので、SIMDやGPU等により高速化されやすい\n",
        "    </li>\n",
        "</ul>\n",
        "\n",
        "これらの特徴は互いにトレードオフの関係となっています。\n",
        "\n",
        "そこで、これらのいいとこどりをしようと考えられたのが、**ミニバッチ (mini-batch)**です。\n",
        "\n",
        "データセットを少数のデータ集合、つまりミニバッチに分割し、このミニバッチごとに学習を行うということです。\n",
        "\n",
        "ミニバッチの利用により、使用メモリ量を抑えつつ、高速なベクトル・行列計算能力を利用することができます。\n",
        "\n",
        "そのため、MLPではミニバッチを利用した学習を行うことが標準的ですし、本講義でもミニバッチを利用した学習方法を採用しています。"
      ]
    }
  ]
}